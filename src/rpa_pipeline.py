# -*- coding: utf-8 -*-
"""RPA-version-icv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bek3HuOZ0u45vRsSEM7dLM-H2fFXAyqp

# Robotic Process Automation para constru√ß√£o de banco de dados multimodal
# Fontes: Azulay (PDF), HAM10000 (imagens), derm7pt (imagens + metadados)
# Estrutura do RPA:
#   ROB√î 1 ‚Üí Extra√ß√£o do PDF (Azulay)
#   ROB√î 2 ‚Üí Processamento HAM10000
#   ROB√î 3 ‚Üí Processamento derm7pt
#   ROB√î 4 ‚Üí Constru√ß√£o do Banco de Dados Unificado
#   ROB√î 5 ‚Üí Valida√ß√£o e Relat√≥rio de Qualidade
# =============================================================================
"""
import subprocess
import sys

pacotes = ["PyMuPDF", "Pillow", "pandas", "tqdm", "unidecode"]
for pacote in pacotes:
    subprocess.check_call([sys.executable, "-m", "pip", "install", pacote, "-q"])

print("‚úÖ Depend√™ncias instaladas com sucesso!")

import fitz  # PyMuPDF
import os
import re
import json
import sqlite3
import hashlib
import csv
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Tuple
from PIL import Image
from io import BytesIO
from tqdm.auto import tqdm
from unidecode import unidecode
import shutil
import warnings
warnings.filterwarnings("ignore")

import kagglehub
ham_path = kagglehub.dataset_download("kmader/skin-cancer-mnist-ham10000")
print(f"‚úÖ HAM10000 baixado em: {ham_path}")

# --- Configura√ß√£o de logging ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger("RPA_Dermatologia")

# --- Caminhos ---
class Config:
    """
    ‚ö†Ô∏è AJUSTE OS CAMINHOS ABAIXO CONFORME SUA ESTRUTURA NO GOOGLE DRIVE/COLAB
    """
    # Raiz do projeto no Google Drive
    DRIVE_ROOT = "/content/drive/MyDrive/IC_Dermatologia"

    # Caminho do PDF do Azulay
    AZULAY_PDF = os.path.join(DRIVE_ROOT, "Dermatologia _Azulay.pdf")

    # Diret√≥rio do HAM10000
    HAM10000_DIR = ham_path
    HAM10000_METADATA = os.path.join(HAM10000_DIR, "HAM10000_metadata.csv")
    HAM10000_IMAGES = ham_path

    # Diret√≥rio do derm7pt
    DERM7PT_DIR = os.path.join(DRIVE_ROOT, "derm7pt")
    DERM7PT_META = os.path.join(DERM7PT_DIR, "meta")
    DERM7PT_IMAGES = os.path.join(DERM7PT_DIR, "images")

    # Sa√≠da do RPA
    OUTPUT_DIR = os.path.join(DRIVE_ROOT, "RPA_Output")
    DB_PATH = os.path.join(OUTPUT_DIR, "dermatologia.db")
    IMAGES_OUTPUT = os.path.join(OUTPUT_DIR, "imagens_extraidas")
    RELATORIO_PATH = os.path.join(OUTPUT_DIR, "relatorio_rpa.json")

    @classmethod
    def setup(cls):
        """Cria diret√≥rios de sa√≠da se n√£o existirem."""
        os.makedirs(cls.OUTPUT_DIR, exist_ok=True)
        os.makedirs(cls.IMAGES_OUTPUT, exist_ok=True)
        os.makedirs(os.path.join(cls.IMAGES_OUTPUT, "azulay"), exist_ok=True)
        os.makedirs(os.path.join(cls.IMAGES_OUTPUT, "ham10000"), exist_ok=True)
        os.makedirs(os.path.join(cls.IMAGES_OUTPUT, "derm7pt"), exist_ok=True)
        logger.info("üìÅ Diret√≥rios de sa√≠da criados/verificados.")

    @classmethod
    def validar(cls):
        """Verifica se os arquivos fonte existem."""
        erros = []
        if not os.path.exists(cls.AZULAY_PDF):
            erros.append(f"‚ùå PDF Azulay n√£o encontrado: {cls.AZULAY_PDF}")
        if not os.path.isdir(cls.HAM10000_DIR):
            erros.append(f"‚ùå Diret√≥rio HAM10000 n√£o encontrado: {cls.HAM10000_DIR}")
        if not os.path.isdir(cls.DERM7PT_DIR):
            erros.append(f"‚ùå Diret√≥rio derm7pt n√£o encontrado: {cls.DERM7PT_DIR}")
        if erros:
            for e in erros:
                logger.error(e)
            logger.warning("‚ö†Ô∏è Ajuste os caminhos na classe Config antes de continuar!")
            return False
        logger.info("‚úÖ Todos os caminhos validados com sucesso!")
        return True

from google.colab import drive
drive.mount("/content/drive")

# Criar diret√≥rios e validar
Config.setup()
caminhos_ok = Config.validar()

if not caminhos_ok:
    print("\n" + "=" * 60)
    print("‚ö†Ô∏è  A√á√ÉO NECESS√ÅRIA:")
    print("Edite a classe Config na c√©lula anterior com os caminhos")
    print("corretos dos seus arquivos no Google Drive.")
    print("=" * 60)

# Defini√ß√£o dos modelos de dados que o RPA vai popular

@dataclass
class SecaoPDF:
    """Representa uma se√ß√£o extra√≠da do livro Azulay."""
    id: str
    capitulo: str
    numero_capitulo: int
    secao: str
    subsecao: str
    texto: str
    pagina_inicio: int
    pagina_fim: int
    nivel_hierarquia: int  # 1=capitulo, 2=secao, 3=subsecao
    palavras_chave: List[str] = field(default_factory=list)
    doencas_mencionadas: List[str] = field(default_factory=list)

@dataclass
class ImagemPDF:
    """Representa uma imagem extra√≠da do Azulay."""
    id: str
    pagina: int
    capitulo: str
    legenda: str
    caminho_arquivo: str
    largura: int
    altura: int
    formato: str
    hash_md5: str
    secao_relacionada: str = ""

@dataclass
class ImagemDataset:
    """Representa uma imagem do HAM10000 ou derm7pt."""
    id: str
    dataset: str  # "ham10000" ou "derm7pt"
    caminho_original: str
    caminho_destino: str
    diagnostico: str
    diagnostico_detalhado: str = ""
    metadados: Dict = field(default_factory=dict)
    hash_md5: str = ""
    largura: int = 0
    altura: int = 0

@dataclass
class RegistroUnificado:
    """Registro final no banco de dados unificado."""
    id: str
    fonte: str  # "azulay_texto", "azulay_imagem", "ham10000", "derm7pt"
    tipo: str  # "texto", "imagem", "texto_imagem"
    conteudo_texto: str = ""
    caminho_imagem: str = ""
    diagnostico: str = ""
    capitulo: str = ""
    secao: str = ""
    metadados_json: str = ""

# ROB√î 1 ‚Äî EXTRA√á√ÉO DO PDF AZULAY
# ===============================================================

print("=" * 60)
print("ü§ñ ROB√î 1: Extraindo conte√∫do do Azulay (PDF)")
print("=" * 60)

class RoboAzulay:
    """
    Rob√¥ de extra√ß√£o autom√°tica do livro Dermatologia de Azulay.

    Pipeline:
    1. Abre o PDF com PyMuPDF
    2. Extrai texto p√°gina por p√°gina
    3. Identifica estrutura hier√°rquica (cap√≠tulos, se√ß√µes, subse√ß√µes)
    4. Extrai imagens e associa com legendas
    5. Identifica termos dermatol√≥gicos e doen√ßas mencionadas
    6. Retorna dados estruturados
    """

    # --- Padr√µes Regex para estrutura do livro ---
    # Ajuste estes padr√µes conforme a formata√ß√£o real do seu PDF
    REGEX_CAPITULO = re.compile(
        r"^(?:Cap[√≠i]tulo|CAP√çTULO|CAP[√çI]TULO)\s*(\d{1,3})\s*[.\-‚Äì‚Äî]?\s*(.+)",
        re.IGNORECASE | re.MULTILINE
    )
    REGEX_SECAO = re.compile(
        r"^([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á\s]{4,60})$",
        re.MULTILINE
    )
    REGEX_SUBSECAO = re.compile(
        r"^(?:#{1,3}\s+)?([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß].{5,80})$",
        re.MULTILINE
    )

    # Padr√µes para detectar legendas de figuras
    REGEX_LEGENDA = re.compile(
        r"(?:Fig(?:ura)?\.?\s*\d+[\.\-]?\d*[\.\-:]?\s*.{5,200})",
        re.IGNORECASE
    )

    # Termos dermatol√≥gicos comuns para indexa√ß√£o
    TERMOS_DERMATOLOGICOS = [
        "melanoma", "carcinoma", "basocelular", "espinocelular", "nevo",
        "dermatite", "eczema", "psor√≠ase", "acne", "ros√°cea", "vitiligo",
        "l√∫pus", "esclerodermia", "p√™nfigo", "penfigoide", "urtic√°ria",
        "micose", "dermatofitose", "candid√≠ase", "herpes", "verruga",
        "molusco", "hansen√≠ase", "leishmaniose", "s√≠filis", "eritema",
        "l√≠quen", "alopecia", "ceratose", "ceratoacantoma", "dermatoscopia",
        "les√£o", "m√°cula", "p√°pula", "n√≥dulo", "ves√≠cula", "bolha",
        "√∫lcera", "placa", "tumor", "cisto", "abscesso", "f√≠stula",
        "prurido", "eritrodermia", "fotossensibilidade", "hiperpigmenta√ß√£o",
        "hipopigmenta√ß√£o", "queloide", "cicatriz", "biopsia",
        "histopatologia", "imunohistoqu√≠mica", "crioterapia", "curetagem",
        "eletrocoagula√ß√£o", "excis√£o", "cirurgia microgr√°fica",
        "fototerapia", "puva", "uvb", "imunossupressor", "corticoide",
        "retin√≥ide", "antif√∫ngico", "antibi√≥tico", "antiviral",
        "melanoc√≠tico", "pigmentada", "vascular", "granulomatosa",
        "inflamat√≥ria", "infecciosa", "autoimune", "neoplasia",
        "benigna", "maligna", "pr√©-maligna", "met√°stase",
        "epiderme", "derme", "hipoderme", "estrato c√≥rneo",
        "fol√≠culo", "gl√¢ndula seb√°cea", "gl√¢ndula sudor√≠para",
        "papilomav√≠rus", "hpv", "aids", "hiv",
        "dermatologia pedi√°trica", "genodermatose", "fotodermatose",
        "farmacodermia", "toxidermia", "angioedema",
        "linfoma cut√¢neo", "sarcoma de kaposi", "doen√ßa de paget",
        "nevus", "displasia", "atipia"
    ]

    def __init__(self, pdf_path: str, output_dir: str):
        self.pdf_path = pdf_path
        self.output_dir = output_dir
        self.doc = None
        self.secoes: List[SecaoPDF] = []
        self.imagens: List[ImagemPDF] = []
        self.estatisticas = {
            "total_paginas": 0,
            "total_secoes": 0,
            "total_imagens": 0,
            "total_capitulos": 0,
            "total_caracteres": 0,
            "total_palavras": 0,
            "termos_encontrados": Counter(),
            "erros": []
        }

    def executar(self) -> Tuple[List[SecaoPDF], List[ImagemPDF], dict]:
        """Pipeline principal do Rob√¥ 1."""
        logger.info("üìñ Abrindo PDF do Azulay...")
        self.doc = fitz.open(self.pdf_path)
        self.estatisticas["total_paginas"] = len(self.doc)
        logger.info(f"   Total de p√°ginas: {len(self.doc)}")

        # Etapa 1: Extra√ß√£o bruta de texto por p√°gina
        logger.info("üìù Etapa 1/4: Extraindo texto bruto...")
        texto_por_pagina = self._extrair_texto_bruto()

        # Etapa 2: Identifica√ß√£o de estrutura hier√°rquica
        logger.info("üèóÔ∏è  Etapa 2/4: Identificando estrutura do livro...")
        self._identificar_estrutura(texto_por_pagina)

        # Etapa 3: Extra√ß√£o de imagens
        logger.info("üñºÔ∏è  Etapa 3/4: Extraindo imagens...")
        self._extrair_imagens(texto_por_pagina)

        # Etapa 4: Enriquecimento (termos, doen√ßas, palavras-chave)
        logger.info("üî¨ Etapa 4/4: Enriquecendo dados com termos dermatol√≥gicos...")
        self._enriquecer_secoes()

        self.doc.close()

        self.estatisticas["total_secoes"] = len(self.secoes)
        self.estatisticas["total_imagens"] = len(self.imagens)

        logger.info(f"‚úÖ Rob√¥ 1 conclu√≠do!")
        logger.info(f"   Se√ß√µes extra√≠das: {len(self.secoes)}")
        logger.info(f"   Imagens extra√≠das: {len(self.imagens)}")
        logger.info(f"   Cap√≠tulos detectados: {self.estatisticas['total_capitulos']}")

        return self.secoes, self.imagens, self.estatisticas

    def _extrair_texto_bruto(self) -> Dict[int, str]:
        """Extrai texto de cada p√°gina do PDF."""
        texto_por_pagina = {}
        for i in tqdm(range(len(self.doc)), desc="Extraindo texto"):
            page = self.doc[i]
            texto = page.get_text("text")
            if texto.strip():
                texto_por_pagina[i] = texto
                self.estatisticas["total_caracteres"] += len(texto)
                self.estatisticas["total_palavras"] += len(texto.split())
        return texto_por_pagina

    def _identificar_estrutura(self, texto_por_pagina: Dict[int, str]):
        """
        Identifica cap√≠tulos, se√ß√µes e subse√ß√µes no texto extra√≠do.

        Estrat√©gia:
        - Busca padr√µes de cap√≠tulos (ex: "Cap√≠tulo 5 - Dermatites")
        - Detecta se√ß√µes por texto em CAIXA ALTA com comprimento adequado
        - Detecta subse√ß√µes por padr√µes de capitaliza√ß√£o
        - Agrupa texto sob cada n√≠vel hier√°rquico
        """
        capitulo_atual = "Introdu√ß√£o"
        num_capitulo = 0
        secao_atual = "Geral"
        subsecao_atual = ""
        pagina_inicio_secao = 0
        texto_acumulado = []
        secao_id_counter = 0

        for pagina, texto in tqdm(
            sorted(texto_por_pagina.items()),
            desc="Analisando estrutura"
        ):
            linhas = texto.split("\n")

            for linha in linhas:
                linha_strip = linha.strip()
                if not linha_strip or len(linha_strip) < 3:
                    continue

                # Detecta novo cap√≠tulo
                match_cap = self.REGEX_CAPITULO.search(linha_strip)
                if match_cap:
                    # Salva se√ß√£o anterior
                    if texto_acumulado:
                        self._salvar_secao(
                            secao_id_counter, capitulo_atual, num_capitulo,
                            secao_atual, subsecao_atual,
                            "\n".join(texto_acumulado),
                            pagina_inicio_secao, pagina
                        )
                        secao_id_counter += 1
                        texto_acumulado = []

                    num_capitulo = int(match_cap.group(1))
                    capitulo_atual = match_cap.group(2).strip()
                    secao_atual = "Introdu√ß√£o"
                    subsecao_atual = ""
                    pagina_inicio_secao = pagina
                    self.estatisticas["total_capitulos"] += 1
                    continue

                # Detecta nova se√ß√£o (CAIXA ALTA)
                if (linha_strip.isupper() and
                    8 < len(linha_strip) < 80 and
                    not linha_strip.startswith("FIG") and
                    not any(c.isdigit() for c in linha_strip[:3])):

                    if texto_acumulado:
                        self._salvar_secao(
                            secao_id_counter, capitulo_atual, num_capitulo,
                            secao_atual, subsecao_atual,
                            "\n".join(texto_acumulado),
                            pagina_inicio_secao, pagina
                        )
                        secao_id_counter += 1
                        texto_acumulado = []

                    secao_atual = linha_strip.title()
                    subsecao_atual = ""
                    pagina_inicio_secao = pagina
                    continue

                # Detecta subse√ß√£o
                match_sub = self.REGEX_SUBSECAO.match(linha_strip)
                if (match_sub and
                    not linha_strip.isupper() and
                    linha_strip[0].isupper() and
                    len(linha_strip) < 80 and
                    not any(p in linha_strip.lower() for p in
                            ["fig.", "tabela", "quadro", "fonte:"])):

                    # Heur√≠stica: se a linha √© curta e capitalizada,
                    # provavelmente √© um t√≠tulo de subse√ß√£o
                    palavras = linha_strip.split()
                    if len(palavras) <= 8:
                        if texto_acumulado:
                            self._salvar_secao(
                                secao_id_counter, capitulo_atual, num_capitulo,
                                secao_atual, subsecao_atual,
                                "\n".join(texto_acumulado),
                                pagina_inicio_secao, pagina
                            )
                            secao_id_counter += 1
                            texto_acumulado = []

                        subsecao_atual = linha_strip
                        pagina_inicio_secao = pagina
                        continue

                # Linha de conte√∫do normal
                texto_acumulado.append(linha_strip)

        # Salva √∫ltima se√ß√£o
        if texto_acumulado:
            self._salvar_secao(
                secao_id_counter, capitulo_atual, num_capitulo,
                secao_atual, subsecao_atual,
                "\n".join(texto_acumulado),
                pagina_inicio_secao,
                max(texto_por_pagina.keys())
            )

    def _salvar_secao(self, idx, capitulo, num_cap, secao, subsecao,
                      texto, pag_ini, pag_fim):
        """Cria e armazena um objeto SecaoPDF."""
        nivel = 1 if secao == "Introdu√ß√£o" and not subsecao else (
            2 if not subsecao else 3
        )
        secao_obj = SecaoPDF(
            id=f"azulay_sec_{idx:05d}",
            capitulo=capitulo,
            numero_capitulo=num_cap,
            secao=secao,
            subsecao=subsecao,
            texto=texto,
            pagina_inicio=pag_ini,
            pagina_fim=pag_fim,
            nivel_hierarquia=nivel
        )
        self.secoes.append(secao_obj)

    def _extrair_imagens(self, texto_por_pagina: Dict[int, str]):
        """
        Extrai imagens do PDF e tenta associar com legendas.

        Estrat√©gia:
        - Para cada p√°gina, lista as imagens presentes
        - Filtra imagens muito pequenas (√≠cones, artefatos)
        - Busca legendas no texto da mesma p√°gina (padr√£o "Fig. X")
        - Salva imagens com nomes descritivos
        """
        img_counter = 0
        capitulo_atual = "Introdu√ß√£o"

        for i in tqdm(range(len(self.doc)), desc="Extraindo imagens"):
            page = self.doc[i]

            # Atualiza cap√≠tulo atual
            if i in texto_por_pagina:
                match = self.REGEX_CAPITULO.search(texto_por_pagina[i])
                if match:
                    capitulo_atual = match.group(2).strip()

            # Lista imagens na p√°gina
            image_list = page.get_images(full=True)

            # Busca legendas na p√°gina
            legendas_pagina = []
            if i in texto_por_pagina:
                legendas_pagina = self.REGEX_LEGENDA.findall(
                    texto_por_pagina[i]
                )

            for img_idx, img_info in enumerate(image_list):
                xref = img_info[0]

                try:
                    base_image = self.doc.extract_image(xref)
                    if not base_image:
                        continue

                    img_bytes = base_image["image"]
                    img_ext = base_image.get("ext", "png")
                    width = base_image.get("width", 0)
                    height = base_image.get("height", 0)

                    # Filtrar imagens muito pequenas (provavelmente √≠cones)
                    if width < 50 or height < 50:
                        continue

                    # Filtrar imagens muito grandes em bytes que s√£o
                    # provavelmente backgrounds
                    if len(img_bytes) < 500:
                        continue

                    # Gerar hash para evitar duplicatas
                    img_hash = hashlib.md5(img_bytes).hexdigest()

                    # Verificar se j√° foi extra√≠da (duplicata)
                    if any(im.hash_md5 == img_hash for im in self.imagens):
                        continue

                    # Associar legenda (se dispon√≠vel)
                    legenda = ""
                    if img_idx < len(legendas_pagina):
                        legenda = legendas_pagina[img_idx]
                    elif legendas_pagina:
                        legenda = legendas_pagina[0]

                    # Salvar imagem
                    nome_arquivo = (
                        f"azulay_p{i:04d}_img{img_counter:04d}.{img_ext}"
                    )
                    caminho_completo = os.path.join(
                        Config.IMAGES_OUTPUT, "azulay", nome_arquivo
                    )

                    with open(caminho_completo, "wb") as f:
                        f.write(img_bytes)

                    # Registrar
                    img_obj = ImagemPDF(
                        id=f"azulay_img_{img_counter:05d}",
                        pagina=i,
                        capitulo=capitulo_atual,
                        legenda=legenda,
                        caminho_arquivo=caminho_completo,
                        largura=width,
                        altura=height,
                        formato=img_ext,
                        hash_md5=img_hash
                    )
                    self.imagens.append(img_obj)
                    img_counter += 1

                except Exception as e:
                    self.estatisticas["erros"].append(
                        f"Erro imagem p√°g {i}, xref {xref}: {str(e)}"
                    )

    def _enriquecer_secoes(self):
        """
        Enriquece as se√ß√µes com:
        - Palavras-chave (termos dermatol√≥gicos encontrados)
        - Doen√ßas mencionadas
        """
        for secao in tqdm(self.secoes, desc="Enriquecendo se√ß√µes"):
            texto_lower = secao.texto.lower()
            texto_normalizado = unidecode(texto_lower)

            # Buscar termos dermatol√≥gicos
            termos_encontrados = []
            doencas = []

            for termo in self.TERMOS_DERMATOLOGICOS:
                termo_normalizado = unidecode(termo.lower())
                if termo_normalizado in texto_normalizado:
                    termos_encontrados.append(termo)
                    self.estatisticas["termos_encontrados"][termo] += 1

                    # Termos que indicam doen√ßas espec√≠ficas
                    if any(ind in termo for ind in [
                        "melanoma", "carcinoma", "dermatite", "psor√≠ase",
                        "acne", "ros√°cea", "vitiligo", "l√∫pus", "p√™nfigo",
                        "urtic√°ria", "micose", "herpes", "hansen√≠ase",
                        "leishmaniose", "s√≠filis", "l√≠quen", "alopecia",
                        "ceratose", "linfoma", "sarcoma", "eczema"
                    ]):
                        doencas.append(termo)

            secao.palavras_chave = list(set(termos_encontrados))
            secao.doencas_mencionadas = list(set(doencas))


# --- EXECUTAR ROB√î 1 ---
if os.path.exists(Config.AZULAY_PDF):
    robo1 = RoboAzulay(Config.AZULAY_PDF, Config.IMAGES_OUTPUT)
    secoes_azulay, imagens_azulay, stats_azulay = robo1.executar()
else:
    logger.warning("‚ö†Ô∏è PDF do Azulay n√£o encontrado. Pulando Rob√¥ 1.")
    secoes_azulay, imagens_azulay, stats_azulay = [], [], {}

# ROB√î 2 ‚Äî PROCESSAMENTO HAM10000
# ===============================================================

print("\n" + "=" * 60)
print("ü§ñ ROB√î 2: Processando dataset HAM10000")
print("=" * 60)

class RoboHAM10000:
    """
    Rob√¥ de processamento autom√°tico do dataset HAM10000.

    HAM10000 cont√©m 10.015 imagens dermatosc√≥picas com 7 categorias:
    - akiec: Ceratose act√≠nica / Carcinoma intraepitelial
    - bcc: Carcinoma basocelular
    - bkl: Les√µes benignas tipo ceratose
    - df: Dermatofibroma
    - mel: Melanoma
    - nv: Nevos melanoc√≠ticos
    - vasc: Les√µes vasculares

    Pipeline:
    1. L√™ metadados do CSV
    2. Valida exist√™ncia das imagens
    3. Extrai dimens√µes e hash de cada imagem
    4. Enriquece com nomes completos em portugu√™s
    5. Retorna dados estruturados
    """

    # Mapeamento de c√≥digos para nomes completos (PT-BR)
    DIAGNOSTICOS_PTBR = {
        "akiec": {
            "nome": "Ceratose Act√≠nica / Carcinoma Intraepitelial",
            "descricao": "Les√£o pr√©-maligna causada por exposi√ß√£o solar cr√¥nica",
            "cid10": "L57.0 / D04"
        },
        "bcc": {
            "nome": "Carcinoma Basocelular",
            "descricao": "Neoplasia maligna mais comum da pele",
            "cid10": "C44"
        },
        "bkl": {
            "nome": "Ceratose Seborreica / Les√£o Benigna",
            "descricao": "Les√£o benigna comum, especialmente em idosos",
            "cid10": "L82"
        },
        "df": {
            "nome": "Dermatofibroma",
            "descricao": "Les√£o benigna fibrosa da derme",
            "cid10": "D23"
        },
        "mel": {
            "nome": "Melanoma",
            "descricao": "Neoplasia maligna de melan√≥citos, alto potencial metast√°tico",
            "cid10": "C43"
        },
        "nv": {
            "nome": "Nevo Melanoc√≠tico",
            "descricao": "Les√£o benigna melanoc√≠tica (pinta comum)",
            "cid10": "D22"
        },
        "vasc": {
            "nome": "Les√£o Vascular",
            "descricao": "Angioma, angioceratoma ou granuloma piog√™nico",
            "cid10": "D18"
        }
    }

    def __init__(self, ham_dir: str, metadata_csv: str, images_dir: str):
        self.ham_dir = ham_dir
        self.metadata_csv = metadata_csv
        self.images_dir = images_dir
        self.registros: List[ImagemDataset] = []
        self.estatisticas = {
            "total_registros_csv": 0,
            "total_imagens_encontradas": 0,
            "total_imagens_ausentes": 0,
            "distribuicao_diagnosticos": Counter(),
            "erros": []
        }

    def executar(self) -> Tuple[List[ImagemDataset], dict]:
        """Pipeline principal do Rob√¥ 2."""

        # Etapa 1: Ler metadados
        logger.info("üìã Etapa 1/3: Lendo metadados do CSV...")
        metadados = self._ler_metadados()

        # Etapa 2: Processar imagens
        logger.info("üñºÔ∏è  Etapa 2/3: Processando imagens...")
        self._processar_imagens(metadados)

        # Etapa 3: Estat√≠sticas
        logger.info("üìä Etapa 3/3: Gerando estat√≠sticas...")
        self._gerar_estatisticas()

        logger.info(f"‚úÖ Rob√¥ 2 conclu√≠do!")
        logger.info(f"   Imagens processadas: {len(self.registros)}")

        return self.registros, self.estatisticas

    def _ler_metadados(self) -> List[dict]:
        """L√™ o CSV de metadados do HAM10000."""
        metadados = []

        # Tenta encontrar o CSV de metadados
        csv_path = self.metadata_csv
        if not os.path.exists(csv_path):
            # Busca recursiva por CSVs no diret√≥rio
            for root, dirs, files in os.walk(self.ham_dir):
                for f in files:
                    if f.endswith(".csv"):
                        csv_path = os.path.join(root, f)
                        logger.info(f"   CSV encontrado: {csv_path}")
                        break

        if not os.path.exists(csv_path):
            logger.error("‚ùå CSV de metadados n√£o encontrado!")
            return metadados

        import pandas as pd
        df = pd.read_csv(csv_path)
        self.estatisticas["total_registros_csv"] = len(df)
        logger.info(f"   {len(df)} registros no CSV")
        logger.info(f"   Colunas: {list(df.columns)}")

        for _, row in df.iterrows():
            metadados.append(row.to_dict())

        return metadados

    def _processar_imagens(self, metadados: List[dict]):
        """Processa cada imagem do dataset."""

        # Mapeia todos os caminhos de imagens dispon√≠veis
        imagens_disponiveis = {}
        for root, dirs, files in os.walk(self.ham_dir):
            for f in files:
                if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")):
                    nome_sem_ext = os.path.splitext(f)[0]
                    imagens_disponiveis[nome_sem_ext] = os.path.join(root, f)

        logger.info(f"   {len(imagens_disponiveis)} imagens encontradas no diret√≥rio")

        for meta in tqdm(metadados, desc="Processando HAM10000"):
            # O campo geralmente √© "image_id" no HAM10000
            image_id = str(meta.get("image_id", meta.get("image", "")))
            dx = str(meta.get("dx", "desconhecido"))

            # Busca arquivo da imagem
            caminho_img = imagens_disponiveis.get(image_id, "")

            if not caminho_img or not os.path.exists(caminho_img):
                self.estatisticas["total_imagens_ausentes"] += 1
                continue

            self.estatisticas["total_imagens_encontradas"] += 1
            self.estatisticas["distribuicao_diagnosticos"][dx] += 1

            # Informa√ß√µes do diagn√≥stico em PT-BR
            info_dx = self.DIAGNOSTICOS_PTBR.get(dx, {
                "nome": dx,
                "descricao": "Diagn√≥stico n√£o mapeado",
                "cid10": ""
            })

            # Hash e dimens√µes
            try:
                with open(caminho_img, "rb") as f:
                    img_hash = hashlib.md5(f.read()).hexdigest()
                img = Image.open(caminho_img)
                largura, altura = img.size
                img.close()
            except Exception as e:
                img_hash = ""
                largura, altura = 0, 0
                self.estatisticas["erros"].append(
                    f"Erro ao processar {image_id}: {str(e)}"
                )

            # Caminho destino (c√≥pia organizada)
            destino = os.path.join(
                Config.IMAGES_OUTPUT, "ham10000",
                f"{dx}_{image_id}.jpg"
            )

            registro = ImagemDataset(
                id=f"ham_{image_id}",
                dataset="ham10000",
                caminho_original=caminho_img,
                caminho_destino=destino,
                diagnostico=dx,
                diagnostico_detalhado=info_dx["nome"],
                metadados={
                    "dx_type": meta.get("dx_type", ""),
                    "age": meta.get("age", ""),
                    "sex": meta.get("sex", ""),
                    "localization": meta.get("localization", ""),
                    "cid10": info_dx["cid10"],
                    "descricao_ptbr": info_dx["descricao"]
                },
                hash_md5=img_hash,
                largura=largura,
                altura=altura
            )
            self.registros.append(registro)

    def _gerar_estatisticas(self):
        """Gera estat√≠sticas do processamento."""
        logger.info(f"   Distribui√ß√£o de diagn√≥sticos:")
        for dx, count in self.estatisticas[
            "distribuicao_diagnosticos"
        ].most_common():
            info = self.DIAGNOSTICOS_PTBR.get(dx, {"nome": dx})
            logger.info(f"     {dx} ({info['nome']}): {count}")


# --- EXECUTAR ROB√î 2 ---
if os.path.isdir(Config.HAM10000_DIR):
    robo2 = RoboHAM10000(
        Config.HAM10000_DIR,
        Config.HAM10000_METADATA,
        Config.HAM10000_IMAGES
    )
    registros_ham, stats_ham = robo2.executar()
else:
    logger.warning("‚ö†Ô∏è Diret√≥rio HAM10000 n√£o encontrado. Pulando Rob√¥ 2.")
    registros_ham, stats_ham = [], {}

# ROB√î 3 ‚Äî PROCESSAMENTO DERM7PT
# ===============================================================

print("\n" + "=" * 60)
print("ü§ñ ROB√î 3: Processando dataset derm7pt")
print("=" * 60)

class RoboDerm7pt:
    """
    Rob√¥ de processamento autom√°tico do dataset derm7pt.

    derm7pt cont√©m imagens dermatosc√≥picas e cl√≠nicas com
    metadados diagn√≥sticos baseados no checklist de 7 pontos.

    Estrutura t√≠pica:
    - images/: diret√≥rio com imagens
    - meta/: diret√≥rio com metadados (CSVs ou similar)

    Pipeline:
    1. Mapeia estrutura do dataset
    2. L√™ metadados dispon√≠veis
    3. Processa imagens com associa√ß√£o de metadados
    4. Enriquece com informa√ß√µes diagn√≥sticas
    """

    # Mapeamento de diagn√≥sticos derm7pt para portugu√™s
    DIAGNOSTICOS_DERM7PT = {
        "basal cell carcinoma": "Carcinoma Basocelular",
        "blue nevus": "Nevo Azul",
        "clark nevus": "Nevo de Clark",
        "combined nevus": "Nevo Combinado",
        "congenital nevus": "Nevo Cong√™nito",
        "dermal nevus": "Nevo D√©rmico",
        "dermatofibroma": "Dermatofibroma",
        "lentigo": "Lentigo",
        "melanoma": "Melanoma",
        "melanosis": "Melanose",
        "miscellaneous": "Miscel√¢nea",
        "recurrent nevus": "Nevo Recorrente",
        "reed or spitz nevus": "Nevo de Reed/Spitz",
        "seborrheic keratosis": "Ceratose Seborreica",
        "vascular lesion": "Les√£o Vascular",
    }

    # Crit√©rios do checklist de 7 pontos
    CRITERIOS_7PT = {
        "pigment_network": "Rede pigmentar",
        "blue_whitish_veil": "V√©u azul-esbranqui√ßado",
        "vascular_structures": "Estruturas vasculares",
        "pigmentation": "Pigmenta√ß√£o",
        "streaks": "Estrias",
        "dots_and_globules": "Pontos e gl√≥bulos",
        "regression_structures": "Estruturas de regress√£o"
    }

    def __init__(self, derm7pt_dir: str, meta_dir: str, images_dir: str):
        self.derm7pt_dir = derm7pt_dir
        self.meta_dir = meta_dir
        self.images_dir = images_dir
        self.registros: List[ImagemDataset] = []
        self.estatisticas = {
            "total_imagens": 0,
            "total_com_metadados": 0,
            "distribuicao_diagnosticos": Counter(),
            "criterios_7pt_stats": Counter(),
            "erros": []
        }

    def executar(self) -> Tuple[List[ImagemDataset], dict]:
        """Pipeline principal do Rob√¥ 3."""

        # Etapa 1: Mapear estrutura do dataset
        logger.info("üóÇÔ∏è  Etapa 1/3: Mapeando estrutura do derm7pt...")
        estrutura = self._mapear_estrutura()

        # Etapa 2: Ler metadados
        logger.info("üìã Etapa 2/3: Lendo metadados...")
        metadados = self._ler_metadados()

        # Etapa 3: Processar imagens
        logger.info("üñºÔ∏è  Etapa 3/3: Processando imagens...")
        self._processar_imagens(metadados, estrutura)

        logger.info(f"‚úÖ Rob√¥ 3 conclu√≠do!")
        logger.info(f"   Imagens processadas: {len(self.registros)}")

        return self.registros, self.estatisticas

    def _mapear_estrutura(self) -> dict:
        """Mapeia a estrutura de diret√≥rios do derm7pt."""
        estrutura = {
            "imagens": {},
            "metadados_files": [],
            "subdirs": []
        }

        # Mapeia imagens
        for root, dirs, files in os.walk(self.derm7pt_dir):
            for d in dirs:
                estrutura["subdirs"].append(os.path.join(root, d))
            for f in files:
                full_path = os.path.join(root, f)
                if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")):
                    nome = os.path.splitext(f)[0]
                    estrutura["imagens"][nome] = full_path
                elif f.lower().endswith((".csv", ".txt", ".json", ".xlsx")):
                    estrutura["metadados_files"].append(full_path)

        logger.info(f"   Imagens encontradas: {len(estrutura['imagens'])}")
        logger.info(f"   Arquivos de metadados: {len(estrutura['metadados_files'])}")
        logger.info(f"   Subdiret√≥rios: {estrutura['subdirs']}")

        return estrutura

    def _ler_metadados(self) -> Dict[str, dict]:
        """L√™ metadados do derm7pt (flex√≠vel para diferentes formatos)."""
        metadados = {}
        import pandas as pd

        # Busca todos os CSVs no diret√≥rio meta e no diret√≥rio raiz
        csv_files = []
        for root, dirs, files in os.walk(self.derm7pt_dir):
            for f in files:
                if f.lower().endswith(".csv"):
                    csv_files.append(os.path.join(root, f))

        for csv_path in csv_files:
            try:
                df = pd.read_csv(csv_path)
                logger.info(
                    f"   Lendo {os.path.basename(csv_path)}: "
                    f"{len(df)} registros, colunas: {list(df.columns)}"
                )

                # Tenta identificar a coluna de ID da imagem
                id_col = None
                for col_name in ["image_id", "derm", "clinic", "case_id",
                                 "image", "filename", "id", "img_id"]:
                    if col_name in df.columns:
                        id_col = col_name
                        break

                if id_col is None and len(df.columns) > 0:
                    id_col = df.columns[0]

                # Tenta identificar a coluna de diagn√≥stico
                dx_col = None
                for col_name in ["diagnosis", "dx", "label", "class",
                                 "diagnostic", "diagnosis_confirm_type"]:
                    if col_name in df.columns:
                        dx_col = col_name
                        break

                for _, row in df.iterrows():
                    img_id_raw = str(row[id_col]) if id_col else ""
                    # Normaliza: "NEL/Nel026.jpg" ‚Üí "Nel026"
                    img_id = os.path.splitext(os.path.basename(img_id_raw))[0]
                    if img_id:
                        row_dict = row.to_dict()
                        row_dict["_dx_col"] = dx_col
                        row_dict["_id_col"] = id_col
                        row_dict["_source_csv"] = os.path.basename(csv_path)
                        metadados[img_id] = row_dict

            except Exception as e:
                self.estatisticas["erros"].append(
                    f"Erro ao ler {csv_path}: {str(e)}"
                )

        logger.info(f"   Total de metadados carregados: {len(metadados)}")
        return metadados

    def _processar_imagens(self, metadados: Dict, estrutura: dict):
        """Processa imagens do derm7pt com metadados associados."""

        for img_nome, img_path in tqdm(
            estrutura["imagens"].items(),
            desc="Processando derm7pt"
        ):
            self.estatisticas["total_imagens"] += 1

            # Busca metadados
            meta = metadados.get(img_nome, {})
            dx_col = meta.get("_dx_col", "")
            diagnostico_raw = str(meta.get(dx_col, "desconhecido")) if dx_col else "desconhecido"
            diagnostico_ptbr = self.DIAGNOSTICOS_DERM7PT.get(
                diagnostico_raw.lower(),
                diagnostico_raw
            )

            if meta:
                self.estatisticas["total_com_metadados"] += 1

            self.estatisticas["distribuicao_diagnosticos"][diagnostico_raw] += 1

            # Extrai crit√©rios de 7 pontos (se dispon√≠veis nos metadados)
            criterios = {}
            for criterio_en, criterio_pt in self.CRITERIOS_7PT.items():
                for col_variant in [criterio_en,
                                    criterio_en.replace("_", " "),
                                    criterio_pt]:
                    if col_variant in meta:
                        valor = meta[col_variant]
                        criterios[criterio_pt] = valor
                        if str(valor).lower() not in ["0", "absent",
                                                       "nan", "none", ""]:
                            self.estatisticas["criterios_7pt_stats"][
                                criterio_pt
                            ] += 1
                        break

            # Hash e dimens√µes
            try:
                with open(img_path, "rb") as f:
                    img_hash = hashlib.md5(f.read()).hexdigest()
                img = Image.open(img_path)
                largura, altura = img.size
                img.close()
            except Exception as e:
                img_hash = ""
                largura, altura = 0, 0

            # Metadados limpos
            meta_limpo = {
                k: v for k, v in meta.items()
                if not k.startswith("_") and str(v) != "nan"
            }
            meta_limpo["criterios_7pt"] = criterios
            meta_limpo["diagnostico_ptbr"] = diagnostico_ptbr

            destino = os.path.join(
                Config.IMAGES_OUTPUT, "derm7pt",
                f"{diagnostico_raw}_{img_nome}.jpg"
            )

            registro = ImagemDataset(
                id=f"d7pt_{img_nome}",
                dataset="derm7pt",
                caminho_original=img_path,
                caminho_destino=destino,
                diagnostico=diagnostico_raw,
                diagnostico_detalhado=diagnostico_ptbr,
                metadados=meta_limpo,
                hash_md5=img_hash,
                largura=largura,
                altura=altura
            )
            self.registros.append(registro)


# --- EXECUTAR ROB√î 3 ---
if os.path.isdir(Config.DERM7PT_DIR):
    robo3 = RoboDerm7pt(
        Config.DERM7PT_DIR,
        Config.DERM7PT_META,
        Config.DERM7PT_IMAGES
    )
    registros_derm7pt, stats_derm7pt = robo3.executar()
else:
    logger.warning("‚ö†Ô∏è Diret√≥rio derm7pt n√£o encontrado. Pulando Rob√¥ 3.")
    registros_derm7pt, stats_derm7pt = [], {}

#  ROB√î 4 ‚Äî CONSTRU√á√ÉO DO BANCO DE DADOS UNIFICADO
# ===============================================================

print("\n" + "=" * 60)
print("ü§ñ ROB√î 4: Construindo Banco de Dados Unificado (SQLite)")
print("=" * 60)

class RoboBancoDeDados:
    """
    Rob√¥ de constru√ß√£o do banco de dados SQLite unificado.

    Esquema do banco:
    - capitulos: estrutura do livro Azulay
    - secoes_texto: conte√∫do textual extra√≠do do Azulay
    - imagens_azulay: imagens extra√≠das do PDF
    - imagens_ham10000: dataset HAM10000
    - imagens_derm7pt: dataset derm7pt
    - termos_dermatologicos: √≠ndice de termos
    - relacao_secao_termo: liga√ß√£o NxN se√ß√£o ‚Üî termo
    - diagnosticos: tabela unificada de diagn√≥sticos
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        self.estatisticas = {
            "tabelas_criadas": 0,
            "registros_inseridos": Counter(),
            "erros": []
        }

    def executar(
        self,
        secoes: List[SecaoPDF],
        imagens_pdf: List[ImagemPDF],
        registros_ham: List[ImagemDataset],
        registros_derm7pt: List[ImagemDataset]
    ) -> dict:
        """Pipeline principal do Rob√¥ 4."""

        # Remover banco anterior se existir
        if os.path.exists(self.db_path):
            os.remove(self.db_path)

        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute("PRAGMA journal_mode=WAL")
        self.conn.execute("PRAGMA synchronous=NORMAL")

        try:
            # Etapa 1: Criar esquema
            logger.info("üèóÔ∏è  Etapa 1/5: Criando esquema do banco...")
            self._criar_esquema()

            # Etapa 2: Inserir dados do Azulay (texto)
            logger.info("üìù Etapa 2/5: Inserindo se√ß√µes do Azulay...")
            self._inserir_secoes(secoes)

            # Etapa 3: Inserir imagens do Azulay
            logger.info("üñºÔ∏è  Etapa 3/5: Inserindo imagens do Azulay...")
            self._inserir_imagens_azulay(imagens_pdf)

            # Etapa 4: Inserir HAM10000
            logger.info("üñºÔ∏è  Etapa 4/5: Inserindo dados do HAM10000...")
            self._inserir_dataset(registros_ham, "ham10000")

            # Etapa 5: Inserir derm7pt
            logger.info("üñºÔ∏è  Etapa 5/5: Inserindo dados do derm7pt...")
            self._inserir_dataset(registros_derm7pt, "derm7pt")

            # Criar √≠ndices
            logger.info("üîç Criando √≠ndices de busca...")
            self._criar_indices()

            self.conn.commit()

        except Exception as e:
            self.estatisticas["erros"].append(str(e))
            logger.error(f"‚ùå Erro: {e}")
            self.conn.rollback()
        finally:
            self.conn.close()

        logger.info(f"‚úÖ Rob√¥ 4 conclu√≠do! Banco salvo em: {self.db_path}")
        for tabela, count in self.estatisticas["registros_inseridos"].items():
            logger.info(f"   {tabela}: {count} registros")

        return self.estatisticas

    def _criar_esquema(self):
        """Cria todas as tabelas do banco de dados."""
        cursor = self.conn.cursor()

        # Tabela de cap√≠tulos
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS capitulos (
                numero INTEGER PRIMARY KEY,
                titulo TEXT NOT NULL,
                total_secoes INTEGER DEFAULT 0,
                total_imagens INTEGER DEFAULT 0,
                pagina_inicio INTEGER,
                pagina_fim INTEGER
            )
        """)

        # Tabela de se√ß√µes de texto (Azulay)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS secoes_texto (
                id TEXT PRIMARY KEY,
                capitulo TEXT,
                numero_capitulo INTEGER,
                secao TEXT,
                subsecao TEXT,
                texto TEXT,
                pagina_inicio INTEGER,
                pagina_fim INTEGER,
                nivel_hierarquia INTEGER,
                palavras_chave TEXT,
                doencas_mencionadas TEXT,
                total_palavras INTEGER,
                total_caracteres INTEGER,
                FOREIGN KEY (numero_capitulo) REFERENCES capitulos(numero)
            )
        """)

        # Tabela de imagens do Azulay
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS imagens_azulay (
                id TEXT PRIMARY KEY,
                pagina INTEGER,
                capitulo TEXT,
                legenda TEXT,
                caminho_arquivo TEXT,
                largura INTEGER,
                altura INTEGER,
                formato TEXT,
                hash_md5 TEXT UNIQUE,
                secao_relacionada TEXT
            )
        """)

        # Tabela HAM10000
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS imagens_ham10000 (
                id TEXT PRIMARY KEY,
                caminho_original TEXT,
                caminho_destino TEXT,
                diagnostico_codigo TEXT,
                diagnostico_nome TEXT,
                diagnostico_descricao TEXT,
                cid10 TEXT,
                dx_type TEXT,
                idade REAL,
                sexo TEXT,
                localizacao TEXT,
                largura INTEGER,
                altura INTEGER,
                hash_md5 TEXT,
                metadados_json TEXT
            )
        """)

        # Tabela derm7pt
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS imagens_derm7pt (
                id TEXT PRIMARY KEY,
                caminho_original TEXT,
                caminho_destino TEXT,
                diagnostico_original TEXT,
                diagnostico_ptbr TEXT,
                criterios_7pt_json TEXT,
                largura INTEGER,
                altura INTEGER,
                hash_md5 TEXT,
                metadados_json TEXT
            )
        """)

        # Tabela unificada de diagn√≥sticos
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diagnosticos (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                codigo TEXT,
                nome_ptbr TEXT,
                nome_en TEXT,
                descricao TEXT,
                cid10 TEXT,
                fonte TEXT,
                UNIQUE(codigo, fonte)
            )
        """)

        # Tabela de termos dermatol√≥gicos
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS termos_dermatologicos (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                termo TEXT UNIQUE,
                total_ocorrencias INTEGER DEFAULT 0
            )
        """)

        # Tabela relacional se√ß√£o ‚Üî termo
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS relacao_secao_termo (
                secao_id TEXT,
                termo_id INTEGER,
                PRIMARY KEY (secao_id, termo_id),
                FOREIGN KEY (secao_id) REFERENCES secoes_texto(id),
                FOREIGN KEY (termo_id) REFERENCES termos_dermatologicos(id)
            )
        """)

        # Tabela de metadados do RPA
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rpa_metadata (
                chave TEXT PRIMARY KEY,
                valor TEXT,
                data_execucao TEXT
            )
        """)

        self.estatisticas["tabelas_criadas"] = 8
        logger.info(f"   {self.estatisticas['tabelas_criadas']} tabelas criadas")

    def _inserir_secoes(self, secoes: List[SecaoPDF]):
        """Insere se√ß√µes do Azulay no banco."""
        cursor = self.conn.cursor()
        capitulos_vistos = set()

        for secao in tqdm(secoes, desc="Inserindo se√ß√µes"):
            # Inserir cap√≠tulo (se novo)
            if secao.numero_capitulo not in capitulos_vistos:
                cursor.execute("""
                    INSERT OR IGNORE INTO capitulos
                    (numero, titulo, pagina_inicio)
                    VALUES (?, ?, ?)
                """, (secao.numero_capitulo, secao.capitulo,
                      secao.pagina_inicio))
                capitulos_vistos.add(secao.numero_capitulo)

            # Inserir se√ß√£o
            cursor.execute("""
                INSERT OR REPLACE INTO secoes_texto
                (id, capitulo, numero_capitulo, secao, subsecao, texto,
                 pagina_inicio, pagina_fim, nivel_hierarquia,
                 palavras_chave, doencas_mencionadas,
                 total_palavras, total_caracteres)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                secao.id,
                secao.capitulo,
                secao.numero_capitulo,
                secao.secao,
                secao.subsecao,
                secao.texto,
                secao.pagina_inicio,
                secao.pagina_fim,
                secao.nivel_hierarquia,
                json.dumps(secao.palavras_chave, ensure_ascii=False),
                json.dumps(secao.doencas_mencionadas, ensure_ascii=False),
                len(secao.texto.split()),
                len(secao.texto)
            ))
            self.estatisticas["registros_inseridos"]["secoes_texto"] += 1

            # Inserir termos e rela√ß√µes
            for termo in secao.palavras_chave:
                cursor.execute("""
                    INSERT OR IGNORE INTO termos_dermatologicos (termo)
                    VALUES (?)
                """, (termo,))
                cursor.execute("""
                    UPDATE termos_dermatologicos
                    SET total_ocorrencias = total_ocorrencias + 1
                    WHERE termo = ?
                """, (termo,))

                # Rela√ß√£o se√ß√£o ‚Üî termo
                cursor.execute(
                    "SELECT id FROM termos_dermatologicos WHERE termo = ?",
                    (termo,)
                )
                row = cursor.fetchone()
                if row:
                    cursor.execute("""
                        INSERT OR IGNORE INTO relacao_secao_termo
                        (secao_id, termo_id) VALUES (?, ?)
                    """, (secao.id, row[0]))

        self.conn.commit()

    def _inserir_imagens_azulay(self, imagens: List[ImagemPDF]):
        """Insere imagens do Azulay no banco."""
        cursor = self.conn.cursor()

        for img in tqdm(imagens, desc="Inserindo imagens Azulay"):
            cursor.execute("""
                INSERT OR REPLACE INTO imagens_azulay
                (id, pagina, capitulo, legenda, caminho_arquivo,
                 largura, altura, formato, hash_md5, secao_relacionada)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                img.id, img.pagina, img.capitulo, img.legenda,
                img.caminho_arquivo, img.largura, img.altura,
                img.formato, img.hash_md5, img.secao_relacionada
            ))
            self.estatisticas["registros_inseridos"]["imagens_azulay"] += 1

        self.conn.commit()

    def _inserir_dataset(self, registros: List[ImagemDataset], dataset: str):
        """Insere registros de um dataset de imagens no banco."""
        cursor = self.conn.cursor()
        tabela = f"imagens_{dataset}"

        for reg in tqdm(registros, desc=f"Inserindo {dataset}"):
            if dataset == "ham10000":
                cursor.execute(f"""
                    INSERT OR REPLACE INTO {tabela}
                    (id, caminho_original, caminho_destino,
                     diagnostico_codigo, diagnostico_nome,
                     diagnostico_descricao, cid10,
                     dx_type, idade, sexo, localizacao,
                     largura, altura, hash_md5, metadados_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    reg.id, reg.caminho_original, reg.caminho_destino,
                    reg.diagnostico, reg.diagnostico_detalhado,
                    reg.metadados.get("descricao_ptbr", ""),
                    reg.metadados.get("cid10", ""),
                    reg.metadados.get("dx_type", ""),
                    reg.metadados.get("age", None),
                    reg.metadados.get("sex", ""),
                    reg.metadados.get("localization", ""),
                    reg.largura, reg.altura, reg.hash_md5,
                    json.dumps(reg.metadados, ensure_ascii=False)
                ))

            elif dataset == "derm7pt":
                criterios = reg.metadados.get("criterios_7pt", {})
                cursor.execute(f"""
                    INSERT OR REPLACE INTO {tabela}
                    (id, caminho_original, caminho_destino,
                     diagnostico_original, diagnostico_ptbr,
                     criterios_7pt_json,
                     largura, altura, hash_md5, metadados_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    reg.id, reg.caminho_original, reg.caminho_destino,
                    reg.diagnostico, reg.diagnostico_detalhado,
                    json.dumps(criterios, ensure_ascii=False),
                    reg.largura, reg.altura, reg.hash_md5,
                    json.dumps(reg.metadados, ensure_ascii=False)
                ))

            self.estatisticas["registros_inseridos"][tabela] += 1

            # Inserir diagn√≥stico na tabela unificada
            cursor.execute("""
                INSERT OR IGNORE INTO diagnosticos
                (codigo, nome_ptbr, nome_en, descricao, fonte)
                VALUES (?, ?, ?, ?, ?)
            """, (
                reg.diagnostico,
                reg.diagnostico_detalhado,
                reg.diagnostico,
                reg.metadados.get("descricao_ptbr", ""),
                dataset
            ))

        self.conn.commit()

    def _criar_indices(self):
        """Cria √≠ndices para otimizar consultas."""
        cursor = self.conn.cursor()
        indices = [
            "CREATE INDEX IF NOT EXISTS idx_secoes_capitulo ON secoes_texto(numero_capitulo)",
            "CREATE INDEX IF NOT EXISTS idx_secoes_doencas ON secoes_texto(doencas_mencionadas)",
            "CREATE INDEX IF NOT EXISTS idx_ham_diagnostico ON imagens_ham10000(diagnostico_codigo)",
            "CREATE INDEX IF NOT EXISTS idx_ham_localizacao ON imagens_ham10000(localizacao)",
            "CREATE INDEX IF NOT EXISTS idx_derm7pt_diagnostico ON imagens_derm7pt(diagnostico_original)",
            "CREATE INDEX IF NOT EXISTS idx_img_azulay_capitulo ON imagens_azulay(capitulo)",
            "CREATE INDEX IF NOT EXISTS idx_termos_ocorrencias ON termos_dermatologicos(total_ocorrencias DESC)",
        ]
        for idx_sql in indices:
            cursor.execute(idx_sql)
        logger.info(f"   {len(indices)} √≠ndices criados")

        # Inserir metadados do RPA
        cursor.execute("""
            INSERT OR REPLACE INTO rpa_metadata (chave, valor, data_execucao)
            VALUES ('versao', '1.0', ?)
        """, (datetime.now().isoformat(),))
        cursor.execute("""
            INSERT OR REPLACE INTO rpa_metadata (chave, valor, data_execucao)
            VALUES ('pipeline', 'RPA Dermatologia IC', ?)
        """, (datetime.now().isoformat(),))

        self.conn.commit()


# --- EXECUTAR ROB√î 4 ---
robo4 = RoboBancoDeDados(Config.DB_PATH)
stats_db = robo4.executar(
    secoes_azulay,
    imagens_azulay,
    registros_ham,
    registros_derm7pt
)

# ROB√î 5 ‚Äî VALIDA√á√ÉO E RELAT√ìRIO DE QUALIDADE
# ===============================================================

print("\n" + "=" * 60)
print("ü§ñ ROB√î 5: Valida√ß√£o e Relat√≥rio de Qualidade")
print("=" * 60)

class RoboValidacao:
    """
    Rob√¥ de valida√ß√£o que verifica a integridade do banco de dados
    e gera um relat√≥rio detalhado da execu√ß√£o do RPA.

    Verifica√ß√µes:
    - Contagem de registros por tabela
    - Integridade referencial
    - Cobertura de diagn√≥sticos
    - Qualidade dos dados textuais
    - Estat√≠sticas gerais
    """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.relatorio = {}

    def executar(
        self,
        stats_azulay: dict,
        stats_ham: dict,
        stats_derm7pt: dict,
        stats_db: dict
    ) -> dict:
        """Pipeline de valida√ß√£o."""

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # 1. Contagem de registros
        logger.info("üìä Verificando contagem de registros...")
        contagens = {}
        tabelas = [
            "capitulos", "secoes_texto", "imagens_azulay",
            "imagens_ham10000", "imagens_derm7pt",
            "diagnosticos", "termos_dermatologicos"
        ]
        for tabela in tabelas:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {tabela}")
                contagens[tabela] = cursor.fetchone()[0]
            except:
                contagens[tabela] = 0

        # 2. Top termos dermatol√≥gicos
        logger.info("üî¨ Top termos dermatol√≥gicos...")
        cursor.execute("""
            SELECT termo, total_ocorrencias
            FROM termos_dermatologicos
            ORDER BY total_ocorrencias DESC
            LIMIT 20
        """)
        top_termos = [
            {"termo": row[0], "ocorrencias": row[1]}
            for row in cursor.fetchall()
        ]

        # 3. Distribui√ß√£o de diagn√≥sticos
        logger.info("üìà Distribui√ß√£o de diagn√≥sticos...")
        dist_ham = {}
        try:
            cursor.execute("""
                SELECT diagnostico_codigo, diagnostico_nome, COUNT(*)
                FROM imagens_ham10000
                GROUP BY diagnostico_codigo
                ORDER BY COUNT(*) DESC
            """)
            dist_ham = {
                row[0]: {"nome": row[1], "total": row[2]}
                for row in cursor.fetchall()
            }
        except:
            pass

        dist_derm7pt = {}
        try:
            cursor.execute("""
                SELECT diagnostico_original, diagnostico_ptbr, COUNT(*)
                FROM imagens_derm7pt
                GROUP BY diagnostico_original
                ORDER BY COUNT(*) DESC
            """)
            dist_derm7pt = {
                row[0]: {"nome": row[1], "total": row[2]}
                for row in cursor.fetchall()
            }
        except:
            pass

        # 4. Estat√≠sticas textuais do Azulay
        logger.info("üìù Estat√≠sticas textuais...")
        stats_texto = {}
        try:
            cursor.execute("""
                SELECT
                    SUM(total_palavras) as total_palavras,
                    SUM(total_caracteres) as total_chars,
                    AVG(total_palavras) as media_palavras_secao,
                    MIN(total_palavras) as min_palavras,
                    MAX(total_palavras) as max_palavras
                FROM secoes_texto
            """)
            row = cursor.fetchone()
            if row:
                stats_texto = {
                    "total_palavras": row[0],
                    "total_caracteres": row[1],
                    "media_palavras_por_secao": round(row[2], 1) if row[2] else 0,
                    "menor_secao_palavras": row[3],
                    "maior_secao_palavras": row[4]
                }
        except:
            pass

        # 5. Qualidade - se√ß√µes com poucos dados
        secoes_vazias = 0
        try:
            cursor.execute(
                "SELECT COUNT(*) FROM secoes_texto WHERE total_palavras < 10"
            )
            secoes_vazias = cursor.fetchone()[0]
        except:
            pass

        conn.close()

        # Montar relat√≥rio
        self.relatorio = {
            "data_execucao": datetime.now().isoformat(),
            "resumo": {
                "total_registros_banco": sum(contagens.values()),
                "contagens_por_tabela": contagens,
            },
            "azulay": {
                "paginas_processadas": stats_azulay.get("total_paginas", 0),
                "capitulos_detectados": stats_azulay.get("total_capitulos", 0),
                "secoes_extraidas": contagens.get("secoes_texto", 0),
                "imagens_extraidas": contagens.get("imagens_azulay", 0),
                "estatisticas_texto": stats_texto,
                "secoes_com_menos_de_10_palavras": secoes_vazias,
                "top_termos": top_termos
            },
            "ham10000": {
                "total_processado": contagens.get("imagens_ham10000", 0),
                "distribuicao_diagnosticos": dist_ham,
                "imagens_ausentes": stats_ham.get(
                    "total_imagens_ausentes", 0
                )
            },
            "derm7pt": {
                "total_processado": contagens.get("imagens_derm7pt", 0),
                "distribuicao_diagnosticos": dist_derm7pt
            },
            "diagnosticos_unificados": contagens.get("diagnosticos", 0),
            "erros": {
                "azulay": stats_azulay.get("erros", []),
                "ham10000": stats_ham.get("erros", []),
                "derm7pt": stats_derm7pt.get("erros", []),
                "banco": stats_db.get("erros", [])
            },
            "qualidade": {
                "score_completude": self._calcular_score(contagens),
                "recomendacoes": self._gerar_recomendacoes(contagens, secoes_vazias)
            }
        }

        # Salvar relat√≥rio
        relatorio_path = Config.RELATORIO_PATH
        with open(relatorio_path, "w", encoding="utf-8") as f:
            json.dump(self.relatorio, f, ensure_ascii=False, indent=2)

        self._imprimir_resumo()

        return self.relatorio

    def _calcular_score(self, contagens: dict) -> float:
        """Calcula score de completude do RPA (0-100)."""
        score = 0
        if contagens.get("secoes_texto", 0) > 0:
            score += 30
        if contagens.get("imagens_azulay", 0) > 0:
            score += 10
        if contagens.get("imagens_ham10000", 0) > 0:
            score += 25
        if contagens.get("imagens_derm7pt", 0) > 0:
            score += 25
        if contagens.get("termos_dermatologicos", 0) > 0:
            score += 10
        return score

    def _gerar_recomendacoes(self, contagens: dict,
                             secoes_vazias: int) -> List[str]:
        """Gera recomenda√ß√µes de melhoria."""
        recs = []
        if contagens.get("secoes_texto", 0) == 0:
            recs.append(
                "‚ö†Ô∏è Nenhuma se√ß√£o extra√≠da do Azulay. "
                "Verifique o caminho do PDF e os padr√µes regex."
            )
        if secoes_vazias > 10:
            recs.append(
                f"‚ö†Ô∏è {secoes_vazias} se√ß√µes com menos de 10 palavras. "
                "Considere ajustar os crit√©rios de detec√ß√£o de se√ß√£o."
            )
        if contagens.get("imagens_ham10000", 0) == 0:
            recs.append(
                "‚ö†Ô∏è Nenhuma imagem HAM10000 processada. "
                "Verifique o caminho do dataset."
            )
        if contagens.get("imagens_derm7pt", 0) == 0:
            recs.append(
                "‚ö†Ô∏è Nenhuma imagem derm7pt processada. "
                "Verifique o caminho do dataset."
            )
        if not recs:
            recs.append("‚úÖ Pipeline executado com todas as fontes de dados.")
        return recs

    def _imprimir_resumo(self):
        """Imprime resumo do relat√≥rio."""
        r = self.relatorio
        print("\n" + "=" * 60)
        print("üìã RELAT√ìRIO FINAL DO RPA")
        print("=" * 60)
        print(f"\nüìÖ Data: {r['data_execucao']}")
        print(f"\nüìä RESUMO DO BANCO DE DADOS:")
        for tabela, count in r["resumo"]["contagens_por_tabela"].items():
            print(f"   ‚Ä¢ {tabela}: {count:,} registros")
        print(f"\n   TOTAL: {r['resumo']['total_registros_banco']:,} registros")

        print(f"\nüìñ AZULAY:")
        az = r["azulay"]
        print(f"   P√°ginas: {az['paginas_processadas']}")
        print(f"   Cap√≠tulos: {az['capitulos_detectados']}")
        print(f"   Se√ß√µes: {az['secoes_extraidas']}")
        print(f"   Imagens: {az['imagens_extraidas']}")
        if az["estatisticas_texto"]:
            st = az["estatisticas_texto"]
            print(f"   Palavras totais: {st.get('total_palavras', 0):,}")
            print(f"   M√©dia palavras/se√ß√£o: {st.get('media_palavras_por_secao', 0)}")

        print(f"\nüñºÔ∏è  HAM10000: {r['ham10000']['total_processado']:,} imagens")
        print(f"üñºÔ∏è  derm7pt: {r['derm7pt']['total_processado']:,} imagens")
        print(f"üè∑Ô∏è  Diagn√≥sticos √∫nicos: {r['diagnosticos_unificados']}")

        score = r["qualidade"]["score_completude"]
        print(f"\nüéØ Score de Completude: {score}/100")

        print(f"\nüí° RECOMENDA√á√ïES:")
        for rec in r["qualidade"]["recomendacoes"]:
            print(f"   {rec}")

        total_erros = sum(
            len(v) for v in r["erros"].values() if isinstance(v, list)
        )
        if total_erros > 0:
            print(f"\n‚ö†Ô∏è Total de erros: {total_erros}")
            for fonte, erros in r["erros"].items():
                if erros:
                    print(f"   {fonte}: {len(erros)} erros")

        print(f"\nüìÅ Banco de dados: {Config.DB_PATH}")
        print(f"üìÅ Relat√≥rio: {Config.RELATORIO_PATH}")
        print("=" * 60)


# --- EXECUTAR ROB√î 5 ---
robo5 = RoboValidacao(Config.DB_PATH)
relatorio_final = robo5.executar(
    stats_azulay if 'stats_azulay' in dir() else {},
    stats_ham if 'stats_ham' in dir() else {},
    stats_derm7pt if 'stats_derm7pt' in dir() else {},
    stats_db if 'stats_db' in dir() else {}
)

# CONSULTAS DE EXEMPLO AO BANCO
# ===============================================================

print("\n" + "=" * 60)
print("üîç CONSULTAS DE EXEMPLO AO BANCO DE DADOS")
print("=" * 60)

def consultar_banco(query: str, params=None, limit=5):
    """Utilit√°rio para consultar o banco."""
    conn = sqlite3.connect(Config.DB_PATH)
    cursor = conn.cursor()
    cursor.execute(query, params or ())
    colunas = [desc[0] for desc in cursor.description]
    resultados = [dict(zip(colunas, row)) for row in cursor.fetchall()]
    conn.close()
    return resultados[:limit]


# Exemplo 1: Buscar se√ß√µes sobre melanoma
print("\n--- Se√ß√µes que mencionam 'melanoma' ---")
resultados = consultar_banco("""
    SELECT id, capitulo, secao, subsecao,
           total_palavras, doencas_mencionadas
    FROM secoes_texto
    WHERE doencas_mencionadas LIKE '%melanoma%'
    ORDER BY total_palavras DESC
    LIMIT 5
""")
for r in resultados:
    print(f"  Cap: {r['capitulo']} | Se√ß√£o: {r['secao']} | "
          f"{r['total_palavras']} palavras")

# Exemplo 2: Distribui√ß√£o HAM10000
print("\n--- Distribui√ß√£o de diagn√≥sticos no HAM10000 ---")
resultados = consultar_banco("""
    SELECT diagnostico_codigo, diagnostico_nome, COUNT(*) as total
    FROM imagens_ham10000
    GROUP BY diagnostico_codigo
    ORDER BY total DESC
""")
for r in resultados:
    print(f"  {r['diagnostico_codigo']}: {r['diagnostico_nome']} ‚Üí "
          f"{r['total']} imagens")

# Exemplo 3: Busca cross-dataset por diagn√≥stico
print("\n--- Busca cross-dataset: 'melanoma' ---")
print("  HAM10000:")
resultados = consultar_banco("""
    SELECT COUNT(*) as total FROM imagens_ham10000
    WHERE diagnostico_codigo = 'mel'
""")
if resultados:
    print(f"    {resultados[0]['total']} imagens de melanoma")

print("  derm7pt:")
resultados = consultar_banco("""
    SELECT COUNT(*) as total FROM imagens_derm7pt
    WHERE diagnostico_original LIKE '%melanoma%'
""")
if resultados:
    print(f"    {resultados[0]['total']} imagens de melanoma")

print("  Azulay (texto):")
resultados = consultar_banco("""
    SELECT COUNT(*) as total FROM secoes_texto
    WHERE doencas_mencionadas LIKE '%melanoma%'
""")
if resultados:
    print(f"    {resultados[0]['total']} se√ß√µes mencionando melanoma")

# EXPORTA√á√ÉO PARA FORMATOS √öTEIS AO TRANSFORMER
# ===============================================================

print("\n" + "=" * 60)
print("üì¶ EXPORTANDO DADOS PARA TREINAMENTO DO TRANSFORMER")
print("=" * 60)

import pandas as pd

def exportar_para_transformer():
    """
    Exporta dados do banco em formatos prontos para o Transformer.

    Gera:
    1. textos_azulay.jsonl - Texto do livro em formato JSONL
    2. imagens_metadata.csv - Metadados unificados de todas as imagens
    3. pares_texto_imagem.jsonl - Pares texto-imagem para treinamento multimodal
    """
    conn = sqlite3.connect(Config.DB_PATH)

    # 1. Textos do Azulay em JSONL
    logger.info("üìù Exportando textos para JSONL...")
    cursor = conn.cursor()
    cursor.execute("""
        SELECT id, capitulo, secao, subsecao, texto,
               palavras_chave, doencas_mencionadas
        FROM secoes_texto
        WHERE total_palavras > 20
        ORDER BY numero_capitulo, pagina_inicio
    """)

    jsonl_path = os.path.join(Config.OUTPUT_DIR, "textos_azulay.jsonl")
    count_textos = 0
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for row in cursor.fetchall():
            registro = {
                "id": row[0],
                "capitulo": row[1],
                "secao": row[2],
                "subsecao": row[3],
                "texto": row[4],
                "palavras_chave": json.loads(row[5]) if row[5] else [],
                "doencas": json.loads(row[6]) if row[6] else [],
                "fonte": "azulay"
            }
            f.write(json.dumps(registro, ensure_ascii=False) + "\n")
            count_textos += 1
    logger.info(f"   {count_textos} registros ‚Üí {jsonl_path}")

    # 2. Metadados unificados de imagens
    logger.info("üñºÔ∏è  Exportando metadados de imagens...")
    df_ham = pd.read_sql_query("""
        SELECT id, 'ham10000' as dataset,
               diagnostico_codigo as diagnostico,
               diagnostico_nome, caminho_original,
               largura, altura, idade, sexo, localizacao
        FROM imagens_ham10000
    """, conn)

    df_derm = pd.read_sql_query("""
        SELECT id, 'derm7pt' as dataset,
               diagnostico_original as diagnostico,
               diagnostico_ptbr as diagnostico_nome,
               caminho_original,
               largura, altura,
               NULL as idade, NULL as sexo, NULL as localizacao
        FROM imagens_derm7pt
    """, conn)

    df_all = pd.concat([df_ham, df_derm], ignore_index=True)
    csv_path = os.path.join(Config.OUTPUT_DIR, "imagens_metadata_unificado.csv")
    df_all.to_csv(csv_path, index=False)
    logger.info(f"   {len(df_all)} registros ‚Üí {csv_path}")

    # 3. Pares texto-imagem (para treinamento multimodal)
    logger.info("üîó Gerando pares texto-imagem...")
    pares_path = os.path.join(Config.OUTPUT_DIR, "pares_texto_imagem.jsonl")
    count_pares = 0

    # Para HAM10000: usa diagn√≥stico como texto
    cursor.execute("""
        SELECT h.id, h.caminho_original, h.diagnostico_nome,
               h.diagnostico_descricao, h.localizacao, h.idade, h.sexo
        FROM imagens_ham10000 h
    """)
    with open(pares_path, "w", encoding="utf-8") as f:
        for row in cursor.fetchall():
            texto = (
                f"Diagn√≥stico: {row[2]}. "
                f"{row[3] or ''} "
                f"Localiza√ß√£o: {row[4] or 'n√£o informada'}. "
                f"Paciente: {row[6] or 'N/A'}, "
                f"idade {row[5] or 'N/A'}."
            ).strip()
            par = {
                "id": row[0],
                "imagem": row[1],
                "texto": texto,
                "dataset": "ham10000",
                "diagnostico": row[2]
            }
            f.write(json.dumps(par, ensure_ascii=False) + "\n")
            count_pares += 1

        # Para derm7pt: inclui crit√©rios de 7 pontos
        cursor.execute("""
            SELECT id, caminho_original, diagnostico_ptbr,
                   criterios_7pt_json
            FROM imagens_derm7pt
        """)
        for row in cursor.fetchall():
            criterios = json.loads(row[3]) if row[3] else {}
            criterios_texto = ", ".join(
                f"{k}: {v}" for k, v in criterios.items()
                if str(v).lower() not in ["0", "absent", "nan", "none", ""]
            )
            texto = (
                f"Diagn√≥stico: {row[2]}. "
                f"Crit√©rios dermatosc√≥picos: {criterios_texto or 'N/A'}."
            ).strip()
            par = {
                "id": row[0],
                "imagem": row[1],
                "texto": texto,
                "dataset": "derm7pt",
                "diagnostico": row[2]
            }
            f.write(json.dumps(par, ensure_ascii=False) + "\n")
            count_pares += 1

    logger.info(f"   {count_pares} pares ‚Üí {pares_path}")

    conn.close()
    logger.info("‚úÖ Exporta√ß√£o conclu√≠da!")

    return {
        "textos_azulay": jsonl_path,
        "imagens_metadata": csv_path,
        "pares_texto_imagem": pares_path,
        "total_textos": count_textos,
        "total_imagens": len(df_all),
        "total_pares": count_pares
    }


exportacao = exportar_para_transformer()

# RESUMO FINAL
# ===============================================================
print("\n" + "=" * 60)
print("üèÅ RPA DERMATOLOGIA ‚Äî EXECU√á√ÉO FINALIZADA")
print("=" * 60)
print(f"""
üìÅ Arquivos gerados em: {Config.OUTPUT_DIR}

   üóÑÔ∏è  Banco de dados:     dermatologia.db
   üìä Relat√≥rio:           relatorio_rpa.json
   üìù Textos (JSONL):      textos_azulay.jsonl
   üñºÔ∏è  Metadados imagens:   imagens_metadata_unificado.csv
   üîó Pares texto-imagem:  pares_texto_imagem.jsonl
   üìÇ Imagens extra√≠das:   imagens_extraidas/

üìå Pr√≥ximos passos:
   1. Verificar o relat√≥rio de qualidade (relatorio_rpa.json)
   2. Ajustar padr√µes regex se necess√°rio (especialmente para o Azulay)
   3. Usar textos_azulay.jsonl para treinar/fine-tunar o Transformer
   4. Usar pares_texto_imagem.jsonl para treinamento multimodal
   5. Implementar seu algoritmo customizado sobre o banco unificado
""")
print("=" * 60)

!pip install wordcloud

# =============================================================================
# üìä VISUALIZA√á√ÉO E AN√ÅLISE DO BANCO DE DADOS ‚Äî IC DERMATOLOGIA
# =============================================================================
# Execute no Google Colab ap√≥s rodar o pipeline RPA
# Gera gr√°ficos profissionais para apresenta√ß√£o √† orientadora
# =============================================================================

# %% ============================================================
# C√âLULA 1: SETUP
# ===============================================================
# !pip install matplotlib seaborn wordcloud -q

import subprocess, sys
for p in ["matplotlib", "seaborn", "wordcloud"]:
    subprocess.check_call([sys.executable, "-m", "pip", "install", p, "-q"])

import sqlite3
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import warnings
warnings.filterwarnings("ignore")

# Estilo profissional
sns.set_theme(style="whitegrid", palette="muted")
plt.rcParams.update({
    "figure.dpi": 150,
    "font.size": 11,
    "font.family": "sans-serif",
    "axes.titlesize": 14,
    "axes.titleweight": "bold",
    "figure.facecolor": "white",
})

# Cores consistentes
CORES = {
    "azulay": "#2E86AB",
    "ham10000": "#A23B72",
    "derm7pt": "#F18F01",
    "destaque": "#C73E1D",
    "fundo": "#F5F5F5",
}
PALETA_DX = sns.color_palette("husl", 10)

# Conex√£o com o banco
# ‚ö†Ô∏è AJUSTE O CAMINHO SE NECESS√ÅRIO
DB_PATH = "/content/drive/MyDrive/IC_Dermatologia/RPA_Output/dermatologia.db"
conn = sqlite3.connect(DB_PATH)

print("‚úÖ Conectado ao banco de dados!")


# %% ============================================================
# C√âLULA 2: CARREGAR DADOS
# ===============================================================
# Se√ß√µes do Azulay
df_secoes = pd.read_sql_query("""
    SELECT id, capitulo, numero_capitulo, secao, subsecao,
           pagina_inicio, pagina_fim, nivel_hierarquia,
           palavras_chave, doencas_mencionadas,
           total_palavras, total_caracteres
    FROM secoes_texto
""", conn)

# Imagens Azulay
df_img_azulay = pd.read_sql_query("""
    SELECT id, pagina, capitulo, legenda, largura, altura, formato
    FROM imagens_azulay
""", conn)

# HAM10000
df_ham = pd.read_sql_query("""
    SELECT id, diagnostico_codigo, diagnostico_nome,
           diagnostico_descricao, cid10,
           dx_type, idade, sexo, localizacao,
           largura, altura
    FROM imagens_ham10000
""", conn)

# derm7pt
df_derm = pd.read_sql_query("""
    SELECT id, diagnostico_original, diagnostico_ptbr,
           criterios_7pt_json, largura, altura, metadados_json
    FROM imagens_derm7pt
""", conn)

# Termos
df_termos = pd.read_sql_query("""
    SELECT termo, total_ocorrencias
    FROM termos_dermatologicos
    ORDER BY total_ocorrencias DESC
""", conn)

# Cap√≠tulos
df_capitulos = pd.read_sql_query("""
    SELECT numero, titulo FROM capitulos ORDER BY numero
""", conn)

print(f"üìä Dados carregados:")
print(f"   Se√ß√µes Azulay: {len(df_secoes):,}")
print(f"   Imagens Azulay: {len(df_img_azulay):,}")
print(f"   HAM10000: {len(df_ham):,}")
print(f"   derm7pt: {len(df_derm):,}")
print(f"   Termos indexados: {len(df_termos):,}")
